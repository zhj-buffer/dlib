{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"../images/DLI Header.png\" alt=\"Header\" style=\"width: 400px;\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with AI on Jetson Nano\n",
    "### Interactive Classification Tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is an interactive data collection, training, and testing tool, provided as part of the NVIDIA Deep Learning Institute (DLI) course, \"Getting Started with AI on Jetson Nano\". It is designed to be run on the Jetson Nano in conjunction with the detailed instructions provided in the online DLI course pages. \n",
    "\n",
    "To start the tool, set the **Camera** and **Task** code cell definitions, then execute all cells.  The interactive tool widgets at the bottom of the notebook will display.  The tool can then be used to gather data, add data, train data, and test data in an iterative and interactive fashion! \n",
    "\n",
    "The explanations in this notebook are intentionally minimal to provide a streamlined experience.  Please see the DLI course pages for detailed information on tool operation and project creation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Camera\n",
    "First, create your camera and set it to `running`.  Uncomment the appropriate camera selection lines, depending on which type of camera you're using (USB or CSI). This cell may take several seconds to execute.\n",
    "<div style=\"border:2px solid black; background-color:#e3ffb3; font-size:12px; padding:8px; margin-top: auto;\"><i>\n",
    "    <h4><i>Tip</i></h4>\n",
    "<p>There can only be one instance of CSICamera or USBCamera at a time.  Before starting a new project and creating a new camera instance, you must first release the existing one. To do so, shut down the notebook's kernel from the JupyterLab pull-down menu: <strong>Kernel->Shutdown Kernel</strong>, then restart it with <strong>Kernel->Restart Kernel</strong>.</p>\n",
    "<ul><code>sudo systemctl restart nvargus-daemon</code> with password:<code>dlinano</code> is included to then force a reset of the camera daemon.</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sudo] password for dlinano: \n",
      "crw-rw----+ 1 root video 81, 0 Jul 11 06:09 /dev/video0\n"
     ]
    }
   ],
   "source": [
    "# Full reset of the camera\n",
    "!echo 'dlinano' | sudo -S systemctl restart nvargus-daemon && printf '\\n'\n",
    "# Check device number\n",
    "!ls -ltrh /dev/video*\n",
    "\n",
    "# USB Camera (Logitech C270 webcam)\n",
    "#from jetcam.usb_camera import USBCamera\n",
    "#camera = USBCamera(width=224, height=224, capture_device=0) # confirm the capture_device number\n",
    "#from jetcam.csi_camera import CSICamera\n",
    "\n",
    "#camera = CSICamera(width=224, height=224)\n",
    "# CSI Camera (Raspberry Pi Camera Module V2)\n",
    "# from jetcam.csi_camera import CSICamera\n",
    "# camera = CSICamera(width=224, height=224)\n",
    "\n",
    "#camera.running = True\n",
    "#print(\"camera created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bef 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6926102252f4564a2b1c1431ec94f32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image(value=b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00\\x00\\x01\\x00\\x01\\x00\\x00\\xff\\xdb\\x00C\\x00\\x02\\x01\\x0â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "camera created\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<traitlets.traitlets.directional_link at 0x7fa4437400>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from jetcam.utils import bgr8_to_jpeg\n",
    "from jetcam.csi_camera import CSICamera\n",
    "import ipywidgets\n",
    "import traitlets\n",
    "\n",
    "camera = CSICamera(width=640, height=480)\n",
    "\n",
    "#image = camera.read()\n",
    "print(\"bef 2\")\n",
    "# Get access to the webcam. The method is different depending on if this is running on a laptop or a Jetson Nano.\n",
    "#if running_on_jetson_nano():\n",
    "# Accessing the camera with OpenCV on a Jetson Nano requires gstreamer with a custom gstreamer source string\n",
    "#    video_capture = cv2.VideoCapture(get_jetson_gstreamer_source(), cv2.CAP_GSTREAMER)\n",
    "#    print(\"gstreamer\")\n",
    "#else:\n",
    "        # Accessing the camera with OpenCV on a laptop just requires passing in the number of the webcam (usually 0)\n",
    "        # Note: You can pass in a filename instead if you want to process a video file instead of a live camera stream\n",
    "#    video_capture = cv2.VideoCapture(0)\n",
    "#    print(\"video0\")\n",
    "\n",
    "#image_widget = ipywidgets.Image(format='jpeg')\n",
    "image = camera.read()\n",
    "image_widget = ipywidgets.Image(format='jpeg')\n",
    "image_widget.value = bgr8_to_jpeg(image)\n",
    "display(image_widget)\n",
    "\n",
    "camera.running = True\n",
    "\n",
    "print(\"camera created\")\n",
    "camera_widget = ipywidgets.Image()\n",
    "traitlets.dlink((camera, 'value'), (camera_widget, 'value'), transform=bgr8_to_jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "Next, define your project `TASK` and what `CATEGORIES` of data you will collect.  You may optionally define space for multiple `DATASETS` with names of your choosing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment/edit the associated lines for the classification task you're building and execute the cell.\n",
    "This cell should only take a few seconds to execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import face_recognition\n",
    "import cv2\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import platform\n",
    "import pickle\n",
    "\n",
    "# Our list of known face encodings and a matching list of metadata about each face.\n",
    "known_face_encodings = []\n",
    "known_face_metadata = []\n",
    "\n",
    "def save_known_faces():\n",
    "    with open(\"known_faces.dat\", \"wb\") as face_data_file:\n",
    "        face_data = [known_face_encodings, known_face_metadata]\n",
    "        pickle.dump(face_data, face_data_file)\n",
    "        print(\"Known faces backed up to disk.\")\n",
    "\n",
    "\n",
    "def load_known_faces():\n",
    "    global known_face_encodings, known_face_metadata\n",
    "\n",
    "    try:\n",
    "        with open(\"known_faces.dat\", \"rb\") as face_data_file:\n",
    "            known_face_encodings, known_face_metadata = pickle.load(face_data_file)\n",
    "            print(\"Known faces loaded from disk.\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(\"No previous face data found - starting with a blank known face list.\")\n",
    "        pass\n",
    "\n",
    "\n",
    "def running_on_jetson_nano():\n",
    "    # To make the same code work on a laptop or on a Jetson Nano, we'll detect when we are running on the Nano\n",
    "    # so that we can access the camera correctly in that case.\n",
    "    # On a normal Intel laptop, platform.machine() will be \"x86_64\" instead of \"aarch64\"\n",
    "    return platform.machine() == \"aarch64\"\n",
    "\n",
    "\n",
    "def get_jetson_gstreamer_source(capture_width=1280, capture_height=720, display_width=1280, display_height=720, framerate=60, flip_method=0):\n",
    "    \"\"\"\n",
    "    Return an OpenCV-compatible video source description that uses gstreamer to capture video from the camera on a Jetson Nano\n",
    "    \"\"\"\n",
    "    return (\n",
    "            f'nvarguscamerasrc ! video/x-raw(memory:NVMM), ' +\n",
    "            f'width=(int){capture_width}, height=(int){capture_height}, ' +\n",
    "            f'format=(string)NV12, framerate=(fraction){framerate}/1 ! ' +\n",
    "            f'nvvidconv flip-method={flip_method} ! ' +\n",
    "            f'video/x-raw, width=(int){display_width}, height=(int){display_height}, format=(string)BGRx ! ' +\n",
    "            'videoconvert ! video/x-raw, format=(string)BGR ! appsink'\n",
    "            )\n",
    "\n",
    "\n",
    "def register_new_face(face_encoding, face_image):\n",
    "    \"\"\"\n",
    "    Add a new person to our list of known faces\n",
    "    \"\"\"\n",
    "    # Add the face encoding to the list of known faces\n",
    "    known_face_encodings.append(face_encoding)\n",
    "    # Add a matching dictionary entry to our metadata list.\n",
    "    # We can use this to keep track of how many times a person has visited, when we last saw them, etc.\n",
    "    print(\"register new face\")\n",
    "    known_face_metadata.append({\n",
    "        \"first_seen\": datetime.now(),\n",
    "        \"first_seen_this_interaction\": datetime.now(),\n",
    "        \"last_seen\": datetime.now(),\n",
    "        \"seen_count\": 1,\n",
    "        \"seen_frames\": 1,\n",
    "        \"face_image\": face_image,\n",
    "        #\"idx\":known_face_metadata.size(),\n",
    "        #\"name\":\"Alan\",\n",
    "    })\n",
    "\n",
    "\n",
    "def lookup_known_face(face_encoding):\n",
    "    \"\"\"\n",
    "    See if this is a face we already have in our face list\n",
    "    \"\"\"\n",
    "    metadata = None\n",
    "\n",
    "    print(\"in lookup_known_face:  len(known_face_encodings): \" +  str(len(known_face_encodings)))\n",
    "    # If our known face list is empty, just return nothing since we can't possibly have seen this face.\n",
    "    if len(known_face_encodings) == 0:\n",
    "        return metadata\n",
    "\n",
    "    # Calculate the face distance between the unknown face and every face on in our known face list\n",
    "    # This will return a floating point number between 0.0 and 1.0 for each known face. The smaller the number,\n",
    "    # the more similar that face was to the unknown face.\n",
    "    face_distances = face_recognition.face_distance(known_face_encodings, face_encoding)\n",
    "    print(face_distances)\n",
    "\n",
    "    # Get the known face that had the lowest distance (i.e. most similar) from the unknown face.\n",
    "    best_match_index = np.argmin(face_distances)\n",
    "    print(\"best_match_index : \" + str(best_match_index), \"face_distances: \", face_distances[best_match_index])\n",
    "\n",
    "#    for known_face_encoding in known_face_encodings:\n",
    "#        match = face_recognition.compare_faces(face_encoding, known_face_encodings)\n",
    "#        if match[0]:\n",
    "#            print(\"the same person\")\n",
    "#        else:\n",
    "#            print(\"match: \", match[0])\n",
    "\n",
    "    # If the face with the lowest distance had a distance under 0.6, we consider it a face match.\n",
    "    # 0.6 comes from how the face recognition model was trained. It was trained to make sure pictures\n",
    "    # of the same person always were less than 0.6 away from each other.\n",
    "    # Here, we are loosening the threshold a little bit to 0.65 because it is unlikely that two very similar\n",
    "    # people will come up to the door at the same time.\n",
    "    if face_distances[best_match_index] < 0.65:\n",
    "        # If we have a match, look up the metadata we've saved for it (like the first time we saw it, etc)\n",
    "        metadata = known_face_metadata[best_match_index]\n",
    "\n",
    "        # Update the metadata for the face so we can keep track of how recently we have seen this face.\n",
    "        metadata[\"last_seen\"] = datetime.now()\n",
    "        metadata[\"seen_frames\"] += 1\n",
    "\n",
    "        # We'll also keep a total \"seen count\" that tracks how many times this person has come to the door.\n",
    "        # But we can say that if we have seen this person within the last 5 minutes, it is still the same\n",
    "        # visit, not a new visit. But if they go away for awhile and come back, that is a new visit.\n",
    "        if datetime.now() - metadata[\"first_seen_this_interaction\"] > timedelta(minutes=5):\n",
    "            metadata[\"first_seen_this_interaction\"] = datetime.now()\n",
    "            metadata[\"seen_count\"] += 1\n",
    "\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No previous face data found - starting with a blank known face list.\n",
      "bef 1\n",
      "here\n",
      "observer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79be534206d04adcb112823c27fa46b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Image(value=b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00\\x00\\x01\\x00\\x01\\x00\\xâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "live_execution_widget created\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from IPython.display import display\n",
    "import time\n",
    "\n",
    "import threading\n",
    "\n",
    "load_known_faces()\n",
    "#main_loop()\n",
    "print(\"bef 1\")\n",
    "\n",
    "state_widget = ipywidgets.ToggleButtons(options=['stop', 'live'], description='state', value='stop')\n",
    "\n",
    "print(\"here\")\n",
    "    # Track how long since we last saved a copy of our known faces to disk as a backup.\n",
    "def live(state_widget, camera_widget,  camera):\n",
    "    #global video_capture\n",
    "    number_of_faces_since_save = 0\n",
    "    count = 1\n",
    "    print(\"thread start.....\")\n",
    "    while state_widget.value == 'live':\n",
    "        #image = camera.value\n",
    "        #ret, frame = video_capture.read()\n",
    "        frame = camera.value\n",
    "        \n",
    "        # Resize frame of video to 1/4 size for faster face recognition processing\n",
    "        small_frame = cv2.resize(frame, (0, 0), fx=0.25, fy=0.25)\n",
    "\n",
    "        # Convert the image from BGR color (which OpenCV uses) to RGB color (which face_recognition uses)\n",
    "        rgb_small_frame = small_frame[:, :, ::-1]\n",
    "\n",
    "        # Find all the face locations and face encodings in the current frame of video\n",
    "        face_locations = face_recognition.face_locations(rgb_small_frame)\n",
    "        face_encodings = face_recognition.face_encodings(rgb_small_frame, face_locations)\n",
    "\n",
    "        # Loop through each detected face and see if it is one we have seen before\n",
    "        # If so, we'll give it a label that we'll draw on top of the video.\n",
    "        face_labels = []\n",
    "        for face_location, face_encoding in zip(face_locations, face_encodings):\n",
    "            # See if this face is in our list of known faces.\n",
    "            metadata = lookup_known_face(face_encoding)\n",
    "\n",
    "            # If we found the face, label the face with some useful information.\n",
    "            if metadata is not None:\n",
    "                time_at_door = datetime.now() - metadata['first_seen_this_interaction']\n",
    "                face_label = f\"Exist for {int(time_at_door.total_seconds())}s\"\n",
    "\n",
    "            # If this is a brand new face, add it to our list of known faces\n",
    "            else:\n",
    "                face_label = \"New visitor!\"\n",
    "\n",
    "                    # Grab the image of the the face from the current frame of video\n",
    "                top, right, bottom, left = face_location\n",
    "                face_image = small_frame[top:bottom, left:right]\n",
    "                face_image = cv2.resize(face_image, (150, 150))\n",
    "\n",
    "                    # Add the new face to our known face data\n",
    "                register_new_face(face_encoding, face_image)\n",
    "\n",
    "            face_labels.append(face_label)\n",
    "\n",
    "            # Draw a box around each face and label each face\n",
    "        for (top, right, bottom, left), face_label in zip(face_locations, face_labels):\n",
    "                # Scale back up face locations since the frame we detected in was scaled to 1/4 size\n",
    "            top *= 4\n",
    "            right *= 4\n",
    "            bottom *= 4\n",
    "            left *= 4\n",
    "\n",
    "                # Draw a box around the face\n",
    "            cv2.rectangle(frame, (left, top), (right, bottom), (150, 0, 255), 2)\n",
    "\n",
    "                # Draw a label with a name below the face\n",
    "            cv2.rectangle(frame, (left, bottom - 35), (right, bottom), (0, 125, 255), cv2.FILLED)\n",
    "            cv2.putText(frame, face_label, (left + 6, bottom - 6), cv2.FONT_HERSHEY_DUPLEX, 0.8, (255, 255, 255), 1)\n",
    "            print(face_label)\n",
    "\n",
    "        print(\"Loop....\")\n",
    "\n",
    "            # Display recent visitor images\n",
    "        number_of_recent_visitors = 0\n",
    "        for metadata in known_face_metadata:\n",
    "                # If we have seen this person in the last minute, draw their image\n",
    "            #cv2.putText(frame, str(metadata['idx']) + metadata[name], (320, 50), cv2.FONT_HERSHEY_DUPLEX, 0.6, (255, 255, 255), 1)\n",
    "            if datetime.now() - metadata[\"last_seen\"] < timedelta(seconds=10) and metadata[\"seen_frames\"] > 5:\n",
    "                # Draw the known face image\n",
    "                x_position = number_of_recent_visitors * 150\n",
    "                frame[30:180, x_position:x_position + 150] = metadata[\"face_image\"]\n",
    "                number_of_recent_visitors += 1\n",
    "\n",
    "                # Label the image with how many times they have visited\n",
    "                visits = metadata['seen_count']\n",
    "                visit_label = f\"{visits} visits\"\n",
    "                if visits == 1:\n",
    "                    visit_label = \"First sight\"\n",
    "                #if metadata['idx'] == 1:\n",
    "                #    cv2.putText(frame, \"Hello Alan!\", (x_position + 200, 440), cv2.FONT_HERSHEY_DUPLEX, 0.6, (255, 255, 255), 1)\n",
    "                cv2.putText(frame, visit_label, (x_position + 10, 170), cv2.FONT_HERSHEY_DUPLEX, 0.6, (255, 255, 255), 1)\n",
    "                #cv2.putText(frame, visit_label + str(metadata['idx']) + metadata[name], (320, 50), cv2.FONT_HERSHEY_DUPLEX, 0.6, (255, 255, 255), 1)\n",
    "                #print(\"number_of_recent_visitors: \" , number_of_recent_visitors)\n",
    "\n",
    "        if number_of_recent_visitors > 0:\n",
    "            cv2.putText(frame, \"Visitors at Front\", (5, 18), cv2.FONT_HERSHEY_DUPLEX, 0.8, (255, 255, 255), 1)\n",
    "\n",
    "        #cv2.imshow('Video', frame)\n",
    "        camera_widget.value = bgr8_to_jpeg(frame)\n",
    "        #if count > 20:\n",
    "        #    cv2.imwrite(\"test.jpg\", frame)\n",
    "\n",
    "        count = count + 1\n",
    "\n",
    "            # Hit 'q' on the keyboard to quit!\n",
    "        #if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        #    save_known_faces()\n",
    "        #    break\n",
    "\n",
    "        # We need to save our known faces back to disk every so often in case something crashes.\n",
    "        if len(face_locations) > 0 and number_of_faces_since_save > 100:\n",
    "            save_known_faces()\n",
    "            number_of_faces_since_save = 0\n",
    "        else:\n",
    "            number_of_faces_since_save += 1\n",
    "\n",
    "        # Release handle to the webcam\n",
    "    #video_capture.release()\n",
    "    #cv2.destroyAllWindows()\n",
    "    #time.sleep(0.001)\n",
    "    \n",
    "\n",
    "\n",
    "def start_live(change):\n",
    "    print(\"get live \")\n",
    "    if change['new'] == 'live':\n",
    "        print(\"live.....\")\n",
    "        execute_thread = threading.Thread(target=live, args=(state_widget, camera_widget, camera))\n",
    "        execute_thread.start()\n",
    "\n",
    "state_widget.observe(start_live, names='value')\n",
    "print(\"observer\")\n",
    "live_execution_widget = ipywidgets.VBox([\n",
    "    ipywidgets.HBox([camera_widget]),\n",
    "    state_widget\n",
    "])\n",
    "\n",
    "display(live_execution_widget)\n",
    "print(\"live_execution_widget created\")\n",
    "\n",
    "\n",
    "\n",
    "#display(camera_widget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thumbs task with ['blocked', 'free'] categories defined\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from dataset import ImageClassificationDataset\n",
    "\n",
    "TASK = 'thumbs'\n",
    "# TASK = 'emotions'\n",
    "# TASK = 'fingers'\n",
    "# TASK = 'diy'\n",
    "\n",
    "CATEGORIES = ['blocked', 'free']\n",
    "# CATEGORIES = ['none', 'happy', 'sad', 'angry']\n",
    "# CATEGORIES = ['1', '2', '3', '4', '5']\n",
    "# CATEGORIES = [ 'diy_1', 'diy_2', 'diy_3']\n",
    "\n",
    "DATASETS = ['C']\n",
    "# DATASETS = ['A', 'B', 'C']\n",
    "\n",
    "TRANSFORMS = transforms.Compose([\n",
    "    transforms.ColorJitter(0.2, 0.2, 0.2, 0.2),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "datasets = {}\n",
    "for name in DATASETS:\n",
    "    datasets[name] = ImageClassificationDataset(TASK + '_' + name, CATEGORIES, TRANSFORMS)\n",
    "    \n",
    "print(\"{} task with {} categories defined\".format(TASK, CATEGORIES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection\n",
    "Execute the cell below to create the data collection tool widget. This cell should only take a few seconds to execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d0107e50e0743348007b2191cf60692",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Image(value=b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00\\x00\\x01\\x00\\x01\\x00\\xâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_collection_widget created\n"
     ]
    }
   ],
   "source": [
    "import ipywidgets\n",
    "import traitlets\n",
    "from IPython.display import display\n",
    "from jetcam.utils import bgr8_to_jpeg\n",
    "\n",
    "# initialize active dataset\n",
    "dataset = datasets[DATASETS[0]]\n",
    "\n",
    "# unobserve all callbacks from camera in case we are running this cell for second time\n",
    "camera.unobserve_all()\n",
    "\n",
    "# create image preview\n",
    "camera_widget = ipywidgets.Image()\n",
    "traitlets.dlink((camera, 'value'), (camera_widget, 'value'), transform=bgr8_to_jpeg)\n",
    "\n",
    "# create widgets\n",
    "dataset_widget = ipywidgets.Dropdown(options=DATASETS, description='dataset')\n",
    "category_widget = ipywidgets.Dropdown(options=dataset.categories, description='category')\n",
    "count_widget = ipywidgets.IntText(description='count')\n",
    "save_widget = ipywidgets.Button(description='add')\n",
    "\n",
    "# manually update counts at initialization\n",
    "count_widget.value = dataset.get_count(category_widget.value)\n",
    "\n",
    "# sets the active dataset\n",
    "def set_dataset(change):\n",
    "    global dataset\n",
    "    dataset = datasets[change['new']]\n",
    "    count_widget.value = dataset.get_count(category_widget.value)\n",
    "dataset_widget.observe(set_dataset, names='value')\n",
    "\n",
    "# update counts when we select a new category\n",
    "def update_counts(change):\n",
    "    count_widget.value = dataset.get_count(change['new'])\n",
    "category_widget.observe(update_counts, names='value')\n",
    "\n",
    "# save image for category and update counts\n",
    "def save(c):\n",
    "    dataset.save_entry(camera.value, category_widget.value)\n",
    "    count_widget.value = dataset.get_count(category_widget.value)\n",
    "save_widget.on_click(save)\n",
    "\n",
    "data_collection_widget = ipywidgets.VBox([\n",
    "    ipywidgets.HBox([camera_widget]), dataset_widget, category_widget, count_widget, save_widget\n",
    "])\n",
    "\n",
    "display(data_collection_widget)\n",
    "print(\"data_collection_widget created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "Execute the following cell to define the neural network and adjust the fully connected layer (`fc`) to match the outputs required for the project.  This cell may take several seconds to execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72280c29ffc7473faeed73faba38eb01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Text(value='8_3.pth', description='model path'), HBox(children=(Button(description='load model'â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model configured and model_widget created\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "\n",
    "device = torch.device('cuda')\n",
    "\n",
    "# ALEXNET\n",
    "# model = torchvision.models.alexnet(pretrained=True)\n",
    "# model.classifier[-1] = torch.nn.Linear(4096, len(dataset.categories))\n",
    "\n",
    "# SQUEEZENET \n",
    "# model = torchvision.models.squeezenet1_1(pretrained=True)\n",
    "# model.classifier[1] = torch.nn.Conv2d(512, len(dataset.categories), kernel_size=1)\n",
    "# model.num_classes = len(dataset.categories)\n",
    "\n",
    "# RESNET 18\n",
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "model.fc = torch.nn.Linear(512, len(dataset.categories))\n",
    "\n",
    "# RESNET 34\n",
    "# model = torchvision.models.resnet34(pretrained=True)\n",
    "# model.fc = torch.nn.Linear(512, len(dataset.categories))\n",
    "    \n",
    "model = model.to(device)\n",
    "\n",
    "model_save_button = ipywidgets.Button(description='save model')\n",
    "model_load_button = ipywidgets.Button(description='load model')\n",
    "model_path_widget = ipywidgets.Text(description='model path', value='8_3.pth')\n",
    "\n",
    "def load_model(c):\n",
    "    model.load_state_dict(torch.load(model_path_widget.value))\n",
    "model_load_button.on_click(load_model)\n",
    "    \n",
    "def save_model(c):\n",
    "    torch.save(model.state_dict(), model_path_widget.value)\n",
    "model_save_button.on_click(save_model)\n",
    "\n",
    "model_widget = ipywidgets.VBox([\n",
    "    model_path_widget,\n",
    "    ipywidgets.HBox([model_load_button, model_save_button])\n",
    "])\n",
    "\n",
    "display(model_widget)\n",
    "print(\"model configured and model_widget created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Live  Execution\n",
    "Execute the cell below to set up the live execution widget.  This cell should only take a few seconds to execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0125f3e2606b4f1999a6c6c1d7784a4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(FloatSlider(value=0.0, description='blocked', max=1.0, orientation='vertical'), â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "live_execution_widget created\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import time\n",
    "from utils import preprocess\n",
    "import torch.nn.functional as F\n",
    "from jetbot import Robot\n",
    "\n",
    "robot = Robot()\n",
    "\n",
    "state_widget = ipywidgets.ToggleButtons(options=['stop', 'live'], description='state', value='stop')\n",
    "prediction_widget = ipywidgets.Text(description='prediction')\n",
    "score_widgets = []\n",
    "for category in dataset.categories:\n",
    "    score_widget = ipywidgets.FloatSlider(min=0.0, max=1.0, description=category, orientation='vertical')\n",
    "    score_widgets.append(score_widget)\n",
    "\n",
    "def live(state_widget, model, camera, prediction_widget, score_widget):\n",
    "    global dataset\n",
    "    while state_widget.value == 'live':\n",
    "        image = camera.value\n",
    "        preprocessed = preprocess(image)\n",
    "        output = model(preprocessed)\n",
    "        output = F.softmax(output, dim=1).detach().cpu().numpy().flatten()\n",
    "        category_index = output.argmax()\n",
    "        prediction_widget.value = dataset.categories[category_index]\n",
    "        #print(\"prediction_widget.value: \", prediction_widget.value)\n",
    "        #print(\"category_index: \" ,category_index)\n",
    "        #print(\"output: \", output)\n",
    "        for i, score in enumerate(list(output)):\n",
    "            #print(\"I: \" , i , \" value: \" , score)\n",
    "            score_widgets[i].value = score\n",
    "            \n",
    "            if score_widgets[1].value > 0.5:\n",
    "                robot.forward(0.4)\n",
    "            else:\n",
    "                robot.left(0.4)\n",
    "            #print(score)\n",
    "        \n",
    "        time.sleep(0.001)\n",
    "            \n",
    "def start_live(change):\n",
    "    if change['new'] == 'live':\n",
    "        execute_thread = threading.Thread(target=live, args=(state_widget, model, camera, prediction_widget, score_widget))\n",
    "        execute_thread.start()\n",
    "\n",
    "state_widget.observe(start_live, names='value')\n",
    "\n",
    "live_execution_widget = ipywidgets.VBox([\n",
    "    ipywidgets.HBox(score_widgets),\n",
    "    prediction_widget,\n",
    "    state_widget\n",
    "])\n",
    "\n",
    "display(live_execution_widget)\n",
    "print(\"live_execution_widget created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Evaluation\n",
    "Execute the following cell to define the trainer, and the widget to control it. This cell may take several seconds to execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73604bb82f2f40f8b2ba96099600f256",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(IntText(value=1, description='epochs'), FloatProgress(value=0.0, description='progress', max=1.â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainer configured and train_eval_widget created\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 1\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=1e-3, momentum=0.9)\n",
    "\n",
    "epochs_widget = ipywidgets.IntText(description='epochs', value=1)\n",
    "eval_button = ipywidgets.Button(description='evaluate')\n",
    "train_button = ipywidgets.Button(description='train')\n",
    "loss_widget = ipywidgets.FloatText(description='loss')\n",
    "accuracy_widget = ipywidgets.FloatText(description='accuracy')\n",
    "progress_widget = ipywidgets.FloatProgress(min=0.0, max=1.0, description='progress')\n",
    "\n",
    "def train_eval(is_training):\n",
    "    global BATCH_SIZE, LEARNING_RATE, MOMENTUM, model, dataset, optimizer, eval_button, train_button, accuracy_widget, loss_widget, progress_widget, state_widget\n",
    "    \n",
    "    try:\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            dataset,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            shuffle=True\n",
    "        )\n",
    "\n",
    "        state_widget.value = 'stop'\n",
    "        train_button.disabled = True\n",
    "        eval_button.disabled = True\n",
    "        time.sleep(1)\n",
    "\n",
    "        if is_training:\n",
    "            model = model.train()\n",
    "        else:\n",
    "            model = model.eval()\n",
    "        while epochs_widget.value > 0:\n",
    "            i = 0\n",
    "            sum_loss = 0.0\n",
    "            error_count = 0.0\n",
    "            for images, labels in iter(train_loader):\n",
    "                # send data to device\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                if is_training:\n",
    "                    # zero gradients of parameters\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                # execute model to get outputs\n",
    "                outputs = model(images)\n",
    "\n",
    "                # compute loss\n",
    "                loss = F.cross_entropy(outputs, labels)\n",
    "\n",
    "                if is_training:\n",
    "                    # run backpropogation to accumulate gradients\n",
    "                    loss.backward()\n",
    "\n",
    "                    # step optimizer to adjust parameters\n",
    "                    optimizer.step()\n",
    "\n",
    "                # increment progress\n",
    "                error_count += len(torch.nonzero(outputs.argmax(1) - labels).flatten())\n",
    "                count = len(labels.flatten())\n",
    "                i += count\n",
    "                sum_loss += float(loss)\n",
    "                progress_widget.value = i / len(dataset)\n",
    "                loss_widget.value = sum_loss / i\n",
    "                accuracy_widget.value = 1.0 - error_count / i\n",
    "                \n",
    "            if is_training:\n",
    "                epochs_widget.value = epochs_widget.value - 1\n",
    "            else:\n",
    "                break\n",
    "    except e:\n",
    "        pass\n",
    "    model = model.eval()\n",
    "\n",
    "    train_button.disabled = False\n",
    "    eval_button.disabled = False\n",
    "    state_widget.value = 'live'\n",
    "    \n",
    "train_button.on_click(lambda c: train_eval(is_training=True))\n",
    "eval_button.on_click(lambda c: train_eval(is_training=False))\n",
    "    \n",
    "train_eval_widget = ipywidgets.VBox([\n",
    "    epochs_widget,\n",
    "    progress_widget,\n",
    "    loss_widget,\n",
    "    accuracy_widget,\n",
    "    ipywidgets.HBox([train_button, eval_button])\n",
    "])\n",
    "\n",
    "display(train_eval_widget)\n",
    "print(\"trainer configured and train_eval_widget created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the Interactive Tool!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interactive tool includes widgets for data collection, training, and testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"../images/classification_tool_key2.png\" alt=\"tool key\" width=500/></center>\n",
    "<br>\n",
    "<center><img src=\"../images/classification_tool_key1.png\" alt=\"tool key\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the cell below to create and display the full interactive widget.  Follow the instructions in the online DLI course pages to build your project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all the widgets into one display\n",
    "all_widget = ipywidgets.VBox([\n",
    "    ipywidgets.HBox([data_collection_widget, live_execution_widget]), \n",
    "    train_eval_widget,\n",
    "    model_widget\n",
    "])\n",
    "\n",
    "display(all_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"../images/DLI Header.png\" alt=\"Header\" style=\"width: 400px;\"/></center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
